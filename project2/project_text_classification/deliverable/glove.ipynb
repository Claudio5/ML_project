{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn import metrics, model_selection\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load our pre-trained word2vec. The first set represents a word2vec with 200 features, the second has 100 features and the last one has 50 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text200 = KeyedVectors.load_word2vec_format('pretrained_word2vec/glove.twitter.27B.200d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text100 = KeyedVectors.load_word2vec_format('pretrained_word2vec/glove.twitter.27B.100d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text50 = KeyedVectors.load_word2vec_format('pretrained_word2vec/glove.twitter.27B.50d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector from the pre-trained word2vec file\n",
    "# If the word is not in vocab returns None\n",
    "def getVecFromWord(word, wv_from_text):\n",
    "    out = None\n",
    "    try:\n",
    "        out = wv_from_text[word]\n",
    "    except:\n",
    "        pass\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getEmbeddedMatrix(input_file, wv_from_text, feature_number):\n",
    "    print('Filename {}'.format(input_file))\n",
    "    \n",
    "    # Compute number of lines\n",
    "    num_lines = sum(1 for line in open(input_file, 'r', encoding=\"utf-8\")) - 1\n",
    "\n",
    "    tweet_matrix = np.zeros((num_lines, feature_number))\n",
    "    y = np.zeros((num_lines,))\n",
    "    \n",
    "    file = csv.reader(open(input_file), delimiter=',')\n",
    "    \n",
    "    for num, line in enumerate(file, 1):\n",
    "        if num != 1:\n",
    "            y[num - 2] = line[2]\n",
    "            \n",
    "            # Log the lines\n",
    "            if not (num % 500000):\n",
    "                print(num)\n",
    "            line_array = []\n",
    "\n",
    "            # Split line into words\n",
    "            for word in line[1].split(\" \"):\n",
    "                for word_split in word.split(\"'\"):\n",
    "                    if(getVecFromWord(word_split, wv_from_text) is not None):\n",
    "                        line_array.append(getVecFromWord(word_split, wv_from_text))\n",
    "\n",
    "            # Is there any words in the vocabulary\n",
    "            if(len(line_array) != 0):\n",
    "                # Append the mean of the words\n",
    "                tmp_array = np.asarray(line_array)\n",
    "                mean_array = np.mean(tmp_array, 0)\n",
    "                tweet_matrix[num - 2][:] = mean_array \n",
    "            # If no word was in the vocabulary add a 0 vector\n",
    "            else:\n",
    "                tweet_matrix[num - 2][:] = np.zeros((feature_number,))\n",
    "    return tweet_matrix, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get our input matrix for our classifier. For that we convert each word to the corresponding vector and average the word vectors over all words of the tweet. We work directly with our cleaned twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename data/crowdai_cleaned_train.csv\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "input_path = 'data/crowdai_cleaned_train.csv'\n",
    "\n",
    "# We define here the word2vec file\n",
    "tx, y = getEmbeddedMatrix(input_path, wv_from_text100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our model we will perform a cross-validation with $k_{fold} = 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'lbfgs')\n",
    "cv_results_logreg = model_selection.cross_validate(logreg, tx, y, cv = 6, return_train_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier()\n",
    "cv_results_ridge = model_selection.cross_validate(ridge, tx, y, cv = 6, return_train_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "cv_results_sgd = model_selection.cross_validate(sgd, tx, y, cv = 6, return_train_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for logistic regression 0.7446576421648238\n",
      "Standard deviation of accuracy for logistic regression 0.00141245625617569\n",
      "Mean accuracy for ridge classifier 0.743555240260417\n",
      "Standard deviation of accuracy for ridge classifier 0.0013717822841246485\n",
      "Mean accuracy for SGD classifier 0.7428794258933974\n",
      "Standard deviation of accuracy for SGD classifier 0.003125925847066541\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy for logistic regression {}'.format(np.mean(cv_results_logreg['test_score'])))\n",
    "print('Standard deviation of accuracy for logistic regression {}'.format(np.std(cv_results_logreg['test_score'])))\n",
    "print('Mean accuracy for ridge classifier {}'.format(np.mean(cv_results_ridge['test_score'])))\n",
    "print('Standard deviation of accuracy for ridge classifier {}'.format(np.std(cv_results_ridge['test_score'])))\n",
    "print('Mean accuracy for SGD classifier {}'.format(np.mean(cv_results_sgd['test_score'])))\n",
    "print('Standard deviation of accuracy for SGD classifier {}'.format(np.std(cv_results_sgd['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
