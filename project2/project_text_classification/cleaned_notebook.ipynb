{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the useful packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from textblob import TextBlob\n",
    "import csv as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used two different datasets as training sets: the “Sentiment140” dataset from Stanford University and the full training set giving to us from CrowdAI. Let's first import the Stanford dataset and delete the extra columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "\n",
    "data_frame_stanford = pd.read_csv(\"./data/stanford_train.csv\",header=None, names=cols,encoding=\"ISO-8859–1\")\n",
    "data_frame_stanford.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "\n",
    "data_frame_stanford.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function which will clean the tweets, by removing @, urls and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(text):\n",
    "    \n",
    "    tok = WordPunctTokenizer() #tokenizer\n",
    "    pat1 = r'@[A-Za-z0-9]+' #users\n",
    "    pat2 = r'https?://[A-Za-z0-9./]+' #urls\n",
    "    combined_pat = r'|'.join((pat1, pat2)) \n",
    "    soup = BeautifulSoup(text, 'lxml') #BeautifulSoup to save computotational \n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat,'', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    #remove if more than 2 consecutrive identive letters\n",
    "    #re.sub(r'((\\w)\\2{2,})', r\"\\2\", text)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    \n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean the Stanford dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 tweets of 1600000 have been cleaned\n",
      "200000 tweets of 1600000 have been cleaned\n",
      "300000 tweets of 1600000 have been cleaned\n",
      "400000 tweets of 1600000 have been cleaned\n",
      "500000 tweets of 1600000 have been cleaned\n",
      "600000 tweets of 1600000 have been cleaned\n",
      "700000 tweets of 1600000 have been cleaned\n",
      "800000 tweets of 1600000 have been cleaned\n",
      "900000 tweets of 1600000 have been cleaned\n",
      "1000000 tweets of 1600000 have been cleaned\n",
      "1100000 tweets of 1600000 have been cleaned\n",
      "1200000 tweets of 1600000 have been cleaned\n",
      "1300000 tweets of 1600000 have been cleaned\n",
      "1400000 tweets of 1600000 have been cleaned\n",
      "1500000 tweets of 1600000 have been cleaned\n",
      "1600000 tweets of 1600000 have been cleaned\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "for i in range(len(data_frame_stanford.text)):\n",
    "    if((i+1)%100000 == 0 ):\n",
    "        print(\"%d tweets of %d have been cleaned\" % (i+1, len(data_frame_stanford.text)))                                                                  \n",
    "    clean_text.append(tweet_cleaner(data_frame_stanford['text'][i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And have a look at the cleaned tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that s a bummer you shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  awww that s a bummer you shoulda got david car...          0\n",
       "1  is upset that he can t update his facebook by ...          0\n",
       "2  i dived many times for the ball managed to sav...          0\n",
       "3     my whole body feels itchy and like its on fire          0\n",
       "4  no it s not behaving at all i m mad why am i h...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_stanford_cleaned = pd.DataFrame(clean_text, columns=['text'])\n",
    "data_frame_stanford_cleaned['sentiment'] = data_frame_stanford.sentiment\n",
    "\n",
    "data_frame_stanford_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdb com very cool to hear old walt intervie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "1599995  just woke up having no school is the best feel...          4\n",
       "1599996  thewdb com very cool to hear old walt intervie...          4\n",
       "1599997  are you ready for your mojo makeover ask me fo...          4\n",
       "1599998  happy th birthday to my boo of alll time tupac...          4\n",
       "1599999                               happy charitytuesday          4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_stanford_cleaned.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"positive sentiment\" is noted 4, let's remplace this value by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdb com very cool to hear old walt intervie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "1599995  just woke up having no school is the best feel...          1\n",
       "1599996  thewdb com very cool to hear old walt intervie...          1\n",
       "1599997  are you ready for your mojo makeover ask me fo...          1\n",
       "1599998  happy th birthday to my boo of alll time tupac...          1\n",
       "1599999                               happy charitytuesday          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_stanford_cleaned.sentiment[data_frame_stanford_cleaned.sentiment == 4] = 1\n",
    "data_frame_stanford_cleaned.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load and clean the CrowdAI dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>glad i dot have taks tomorrow ! ! thankful sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i could actually kill that girl i'm so sorry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i find that very hard to believe im afraid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
       "1          0  glad i dot have taks tomorrow ! ! thankful sta...\n",
       "2          0  1-3 vs celtics in the regular season = were fu...\n",
       "3          0   i could actually kill that girl i'm so sorry ...\n",
       "4          0         i find that very hard to believe im afraid"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['text', 'sentiment']\n",
    "\n",
    "#Load negative sentiment training set\n",
    "neg = pd.read_csv('data/train_neg_full.txt',sep= \"\\n\" ,header=None, names=cols)\n",
    "neg = neg.loc[:, cols[::-1]]\n",
    "neg['sentiment']= 0\n",
    "\n",
    "#Load positive sentiment training set\n",
    "pos = pd.read_csv('data/train_pos_full.txt',sep= \"\\n\" ,header=None, names=cols)\n",
    "pos = pos.loc[:, cols[::-1]]\n",
    "pos['sentiment']= 1\n",
    "\n",
    "#Merge and delete <usr> and <url>\n",
    "frames = [neg, pos] \n",
    "data_frame_crowdai = pd.concat(frames, ignore_index = True)\n",
    "data_frame_crowdai = data_frame_crowdai.drop_duplicates(['text'], keep='first')\n",
    "data_frame_crowdai['text'] = data_frame_crowdai['text'].str.replace('<user>', '')\n",
    "data_frame_crowdai['text'] = data_frame_crowdai['text'].str.replace('<url>', '')\n",
    "data_frame_crowdai['text'] = data_frame_crowdai['text'].str.replace('#', '')\n",
    "data_frame_crowdai = data_frame_crowdai.reset_index(drop=True)\n",
    "\n",
    "data_frame_crowdai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And clean it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 tweets of 2226938 have been cleaned\n",
      "200000 tweets of 2226938 have been cleaned\n",
      "300000 tweets of 2226938 have been cleaned\n",
      "400000 tweets of 2226938 have been cleaned\n",
      "500000 tweets of 2226938 have been cleaned\n",
      "600000 tweets of 2226938 have been cleaned\n",
      "700000 tweets of 2226938 have been cleaned\n",
      "800000 tweets of 2226938 have been cleaned\n",
      "900000 tweets of 2226938 have been cleaned\n",
      "1000000 tweets of 2226938 have been cleaned\n",
      "1100000 tweets of 2226938 have been cleaned\n",
      "1200000 tweets of 2226938 have been cleaned\n",
      "1300000 tweets of 2226938 have been cleaned\n",
      "1400000 tweets of 2226938 have been cleaned\n",
      "1500000 tweets of 2226938 have been cleaned\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-49fe5c795d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d tweets of %d have been cleaned\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame_crowdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame_crowdai\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-988e9f5996a5>\u001b[0m in \u001b[0;36mtweet_cleaner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'https?://[A-Za-z0-9./]+'\u001b[0m \u001b[0;31m#urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcombined_pat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#BeautifulSoup to save computotational\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstripped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_pat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._BaseParser._getPushParserContext\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._BaseParser._createContext\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._setTarget\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1117\u001b[0m                                        \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                                        \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m                                        sigcls=Signature)\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;31m# Most of the times 'signature' will raise ValueError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2193\u001b[0m             \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m             sigcls=sigcls)\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_signature_from_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_signature_is_builtin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func)\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0mpos_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_varnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m     \u001b[0mpositional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpos_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0mkeyword_only_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_kwonlyargcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[0mkeyword_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeyword_only_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "for i in range(len(data_frame_crowdai.text)):\n",
    "    if((i+1)%100000 == 0 ):\n",
    "        print(\"%d tweets of %d have been cleaned\" % (i+1, len(data_frame_crowdai.text)))                                                                  \n",
    "    clean_text.append(tweet_cleaner(data_frame_crowdai['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack difficulty of object disassem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow thankful startho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vs celtics in the regular season were fucked i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i could actually kill that girl i m so sorry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i find that very hard to believe im afraid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  vinco tresorpack difficulty of object disassem...          0\n",
       "1     glad i dot have taks tomorrow thankful startho          0\n",
       "2  vs celtics in the regular season were fucked i...          0\n",
       "3       i could actually kill that girl i m so sorry          0\n",
       "4         i find that very hard to believe im afraid          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_crowdai_cleaned = pd.DataFrame(clean_text, columns = ['text'])\n",
    "data_frame_crowdai_cleaned['sentiment'] = data_frame_crowdai.sentiment\n",
    "\n",
    "# Convert positive labels from 4 to 1\n",
    "data_frame_crowdai_cleaned.sentiment[data_frame_crowdai_cleaned.sentiment == 4] = 1\n",
    "data_frame_crowdai_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge both datasets and remove duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2142594 entries, 0 to 2142593\n",
      "Data columns (total 2 columns):\n",
      "text         object\n",
      "sentiment    int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 32.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#data_frame_tot = pd.Series.append(data_frame_stanford_cleaned,data_frame_crowdai_cleaned)\n",
    "data_frame_tot = data_frame_crowdai_cleaned\n",
    "data_frame_tot = data_frame_tot.drop_duplicates(['text'], keep = 'first')\n",
    "data_frame_tot = data_frame_tot.reset_index(drop = True)\n",
    "data_frame_tot.dropna(inplace = True)\n",
    "data_frame_tot.reset_index(drop = True, inplace = True)\n",
    "data_frame_tot.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save it as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_tot.to_csv('./data/crowdai_cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>vinco tresorpack difficulty of object disassem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>glad i dot have taks tomorrow thankful startho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vs celtics in the regular season were fucked i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i could actually kill that girl i m so sorry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i find that very hard to believe im afraid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  sentiment\n",
       "0           0  vinco tresorpack difficulty of object disassem...          0\n",
       "1           1     glad i dot have taks tomorrow thankful startho          0\n",
       "2           2  vs celtics in the regular season were fucked i...          0\n",
       "3           3       i could actually kill that girl i m so sorry          0\n",
       "4           4         i find that very hard to believe im afraid          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv('./data/crowdai_cleaned_train.csv')\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 2099742 entries with 50.38% negative, 49.62% positive\n",
      "Validation set has total 21426 entries with 49.90% negative, 50.10% positive\n",
      "Test set has total 21426 entries with 50.22% negative, 49.78% positive\n"
     ]
    }
   ],
   "source": [
    "x = data_frame.text.astype('U')\n",
    "y = data_frame.sentiment\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state = SEED)\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train), (len(x_train[y_train == 0]) / (len(x_train)*1.))*100, (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),(len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,(len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),(len(x_test[y_test == 0]) / (len(x_test)*1.))*100,(len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check TextBlob build-in classifier result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 57.46%\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "          predicted_positive  predicted_negative\n",
      "positive                9611                1123\n",
      "negative                7992                2700\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.25      0.37     10692\n",
      "          1       0.55      0.90      0.68     10734\n",
      "\n",
      "avg / total       0.63      0.57      0.53     21426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tbresult = [TextBlob(i).sentiment.polarity for i in x_validation]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]\n",
    "conmat = np.array(confusion_matrix(y_validation, tbpred, labels=[1,0]))\n",
    "confusion = pd.DataFrame(conmat, index=['positive', 'negative'],\n",
    "                         columns=['predicted_positive','predicted_negative'])\n",
    "print(\"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_validation, tbpred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion)\n",
    "print(\"-\"*80)\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_validation, tbpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())\n",
    "    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print(\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)\n",
    "    return accuracy, train_test_time\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "#Regressiontype\n",
    "linear_classifier = SGDClassifier()\n",
    "\n",
    "n_features = np.arange(70000,130001,10000)\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=linear_classifier):\n",
    "    result = []\n",
    "    print(classifier)\n",
    "    print(\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Validation result for {} features\".format(n))\n",
    "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit(x)\n",
    "neg_doc_matrix = cvec.transform(x[data_frame.sentiment == 0])\n",
    "pos_doc_matrix = cvec.transform(x[data_frame.sentiment == 1])\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR TRIGRAM WITH STOP WORDS\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 70000 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 50.10%\n",
      "accuracy score: 81.73%\n",
      "model is 31.63% more accurate than null accuracy\n",
      "train and test time: 232.67s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 80000 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 50.10%\n",
      "accuracy score: 81.82%\n",
      "model is 31.72% more accurate than null accuracy\n",
      "train and test time: 244.22s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 90000 features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8b720af42f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_result_tg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnfeature_accuracy_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-f6da6365c1da>\u001b[0m in \u001b[0;36mnfeature_accuracy_checker\u001b[0;34m(vectorizer, n_features, stop_words, ngram_range, classifier)\u001b[0m\n\u001b[1;32m     37\u001b[0m         ])\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation result for {} features\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mnfeature_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtt_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchecker_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfeature_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtt_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f6da6365c1da>\u001b[0m in \u001b[0;36maccuracy_summary\u001b[0;34m(pipeline, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnull_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentiment_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_test_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    888\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg = nfeature_accuracy_checker(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram')\n",
    "plt.title(\"3gram test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('158INVERSE.csv')\n",
    "df.Prediction[df.Prediction == 1] = -2\n",
    "df.Prediction[df.Prediction == -1] = 1\n",
    "df.Prediction[df.Prediction == -2] = -1\n",
    "df.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
