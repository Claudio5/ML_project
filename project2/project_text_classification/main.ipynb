{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",header=None, names=cols,encoding=\"ISO-8859–1\")\n",
    "\n",
    "# above line will be different depending on where you saved your data, and your file name\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all....\n",
       "5          0                      @Kwesidei not the whole crew \n",
       "6          0                                        Need a hug \n",
       "7          0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8          0               @Tatiana_K nope they didn't have it \n",
       "9          0                          @twittera que me muera ? "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sentiment == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>4</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>4</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>4</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>4</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>4</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800005</th>\n",
       "      <td>4</td>\n",
       "      <td>@ProductOfFear You can tell him that I just bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800006</th>\n",
       "      <td>4</td>\n",
       "      <td>@r_keith_hill Thans for your response. Ihad al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800007</th>\n",
       "      <td>4</td>\n",
       "      <td>@KeepinUpWKris I am so jealous, hope you had a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800008</th>\n",
       "      <td>4</td>\n",
       "      <td>@tommcfly ah, congrats mr fletcher for finally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800009</th>\n",
       "      <td>4</td>\n",
       "      <td>@e4VoIP I RESPONDED  Stupid cat is helping me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                               text\n",
       "800000          4       I LOVE @Health4UandPets u guys r the best!! \n",
       "800001          4  im meeting up with one of my besties tonight! ...\n",
       "800002          4  @DaRealSunisaKim Thanks for the Twitter add, S...\n",
       "800003          4  Being sick can be really cheap when it hurts t...\n",
       "800004          4    @LovesBrooklyn2 he has that effect on everyone \n",
       "800005          4  @ProductOfFear You can tell him that I just bu...\n",
       "800006          4  @r_keith_hill Thans for your response. Ihad al...\n",
       "800007          4  @KeepinUpWKris I am so jealous, hope you had a...\n",
       "800008          4  @tommcfly ah, congrats mr fletcher for finally...\n",
       "800009          4  @e4VoIP I RESPONDED  Stupid cat is helping me ..."
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sentiment == 4].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_shape': (1600000, 3),\n",
      " 'pre_clean_len': {'description': 'Length of the tweet before cleaning',\n",
      "                   'type': dtype('int64')},\n",
      " 'sentiment': {'description': 'sentiment class - 0:negative, 1:positive',\n",
      "               'type': dtype('int64')},\n",
      " 'text': {'description': 'tweet text', 'type': dtype('O')}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "data_dict = {\n",
    "    'sentiment':{\n",
    "        'type':df.sentiment.dtype,\n",
    "        'description':'sentiment class - 0:negative, 1:positive'\n",
    "    },\n",
    "    'text':{\n",
    "        'type':df.text.dtype,\n",
    "        'description':'tweet text'\n",
    "    },\n",
    "    'pre_clean_len':{\n",
    "        'type':df.pre_clean_len.dtype,\n",
    "        'description':'Length of the tweet before cleaning'\n",
    "    },\n",
    "    'dataset_shape':df.shape\n",
    "}\n",
    "pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 299,
       "width": 350
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>Awwh babs... you look so sad underneith that s...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>Tuesdayï¿½ll start with reflection ï¿½n then a...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>Whinging. My client&amp;amp;boss don't understand ...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0</td>\n",
       "      <td>@TheLeagueSF Not Fun &amp;amp; Furious? The new ma...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>#3 woke up and was having an accident - &amp;quot;...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0</td>\n",
       "      <td>My bathtub drain is fired: it haz 1 job 2 do, ...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0</td>\n",
       "      <td>pears &amp;amp; Brie, bottle of Cabernet, and &amp;quo...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0</td>\n",
       "      <td>Have an invite for &amp;quot;Healthy Dining&amp;quot; ...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0</td>\n",
       "      <td>Damnit I was really digging this season of Rea...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>0</td>\n",
       "      <td>Why do I keep looking...I know that what I rea...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text  \\\n",
       "213           0  Awwh babs... you look so sad underneith that s...   \n",
       "226           0  Tuesdayï¿½ll start with reflection ï¿½n then a...   \n",
       "279           0  Whinging. My client&amp;boss don't understand ...   \n",
       "343           0  @TheLeagueSF Not Fun &amp; Furious? The new ma...   \n",
       "400           0  #3 woke up and was having an accident - &quot;...   \n",
       "464           0  My bathtub drain is fired: it haz 1 job 2 do, ...   \n",
       "492           0  pears &amp; Brie, bottle of Cabernet, and &quo...   \n",
       "747           0  Have an invite for &quot;Healthy Dining&quot; ...   \n",
       "957           0  Damnit I was really digging this season of Rea...   \n",
       "1064          0  Why do I keep looking...I know that what I rea...   \n",
       "\n",
       "      pre_clean_len  \n",
       "213             142  \n",
       "226             141  \n",
       "279             145  \n",
       "343             145  \n",
       "400             144  \n",
       "464             146  \n",
       "492             150  \n",
       "747             141  \n",
       "957             141  \n",
       "1064            141  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 140].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whinging. My client&amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;reviewed correctly. \""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[279]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whinging. My client&boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&reviewed correctly. \n"
     ]
    }
   ],
   "source": [
    "example1 = BeautifulSoup(df.text[279], 'lxml')\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@TheLeagueSF Not Fun &amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp; clamped down '"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Not Fun &amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp; clamped down '"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'@[A-Za-z0-9]+','',df.text[343])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('https?://[A-Za-z0-9./]+','',df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuesdayï¿½ll start with reflection ï¿½n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers '"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing = df.text[226].decode(\"utf-8-sig\")\n",
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' machineplay I m so sorry you re having to go through this  Again    therapyfail'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[^a-zA-Z]\", \" \", df.text[175])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awww that s a bummer you shoulda got david carr of third day to do it d',\n",
       " 'is upset that he can t update his facebook by texting it and might cry as a result school today also blah',\n",
       " 'i dived many times for the ball managed to save the rest go out of bounds',\n",
       " 'my whole body feels itchy and like its on fire',\n",
       " 'no it s not behaving at all i m mad why am i here because i can t see you all over there',\n",
       " 'not the whole crew',\n",
       " 'need a hug',\n",
       " 'hey long time no see yes rains a bit only a bit lol i m fine thanks how s you',\n",
       " 'k nope they didn t have it',\n",
       " 'que me muera',\n",
       " 'spring break in plain city it s snowing',\n",
       " 'i just re pierced my ears',\n",
       " 'i couldn t bear to watch it and i thought the ua loss was embarrassing',\n",
       " 'it it counts idk why i did either you never talk to me anymore',\n",
       " 'i would ve been the first but i didn t have a gun not really though zac snyder s just a doucheclown',\n",
       " 'i wish i got to watch it with you i miss you and how was the premiere',\n",
       " 'hollis death scene will hurt me severely to watch on film wry is directors cut not out now',\n",
       " 'about to file taxes',\n",
       " 'ahh ive always wanted to see rent love the soundtrack',\n",
       " 'oh dear were you drinking out of the forgotten table drinks',\n",
       " 'i was out most of the day so didn t get much done',\n",
       " 'one of my friend called me and asked to meet with her at mid valley today but i ve no time sigh',\n",
       " 'barista i baked you a cake but i ated it',\n",
       " 'this week is not going as i had hoped',\n",
       " 'blagh class at tomorrow',\n",
       " 'i hate when i have to call and wake people up',\n",
       " 'just going to cry myself to sleep after watching marley and me',\n",
       " 'im sad now miss lilly',\n",
       " 'ooooh lol that leslie and ok i won t do it again so leslie won t get mad again',\n",
       " 'meh almost lover is the exception this track gets me depressed every time',\n",
       " 'some hacked my account on aim now i have to make a new one',\n",
       " 'i want to go to promote gear and groove but unfornately no ride there i may b going to the one in anaheim in may though',\n",
       " 'thought sleeping in was an option tomorrow but realizing that it now is not evaluations in the morning and work in the afternoon',\n",
       " 'awe i love you too am here i miss you',\n",
       " 'i cry my asian eyes to sleep at night',\n",
       " 'ok i m sick and spent an hour sitting in the shower cause i was too sick to stand and held back the puke like a champ bed now',\n",
       " 'ill tell ya the story later not a good day and ill be workin for like three more hours',\n",
       " 'sorry bed time came here gmt',\n",
       " 'i don t either its depressing i don t think i even want to know about the kids in suitcases',\n",
       " 'bed class work gym or then class another day that s gonna fly by i miss my girlfriend',\n",
       " 'really don t feel like getting up today but got to study to for tomorrows practical exam',\n",
       " 'he s the reason for the teardrops on my guitar the only one who has enough of me to break my heart',\n",
       " 'sad sad sad i don t know why but i hate this feeling i wanna sleep and i still can t',\n",
       " 'awww i soo wish i was there to see you finally comfortable im sad that i missed it',\n",
       " 'falling asleep just heard about that tracy girl s body being found how sad my heart breaks for that family',\n",
       " 'yay i m happy for you with your job but that also means less time for me and you',\n",
       " 'just checked my user timeline on my blackberry it looks like the twanking is still happening are ppl still having probs w bgs and uids',\n",
       " 'oh man was ironing s fave top to wear to a meeting burnt it',\n",
       " 'is strangely sad about lilo and samro breaking up',\n",
       " 'oh i m so sorry i didn t think about that before retweeting',\n",
       " 'broadband plan a massive broken promise via www diigo com tautao still waiting for broadband we are',\n",
       " 'wow tons of replies from you may have to unfollow so i can see my friends tweets you re scrolling the feed a lot',\n",
       " 'our duck and chicken are taking wayyy too long to hatch',\n",
       " 'put vacation photos online a few yrs ago pc crashed and now i forget the name of the site',\n",
       " 'i need a hug',\n",
       " 'not sure what they are only that they are pos as much as i want to i dont think can trade away company assets sorry andy',\n",
       " 'i hate when that happens',\n",
       " 'i have a sad feeling that dallas is not going to show up i gotta say though you d think more shows would use music from the game mmm',\n",
       " 'ugh degrees tomorrow',\n",
       " 'where did u move to i thought u were already in sd hmmm random u found me glad to hear yer doing well',\n",
       " 'i miss my ps it s out of commission wutcha playing have you copped blood on the sand',\n",
       " 'just leaving the parking lot of work',\n",
       " 'the life is cool but not for me',\n",
       " 'sadly though i ve never gotten to experience the post coitus cigarette before and now i never will',\n",
       " 'i had such a nice day too bad the rain comes in tomorrow at am',\n",
       " 'too bad i won t be around i lost my job and can t even pay my phone bill lmao aw shucks',\n",
       " 'damm back to school tomorrow',\n",
       " 'mo jobs no money how in the hell is min wage here f n clams an hour',\n",
       " 'not forever see you soon',\n",
       " 'algonquin agreed i saw the failwhale allllll day today',\n",
       " 'oh haha dude i dont really look at em unless someone says hey i added you sorry i m so terrible at that i need a pop up',\n",
       " 'i m sure you re right i need to start working out with you and the nikster or jared at least',\n",
       " 'i really hate how people diss my bands trace is clearly not ugly',\n",
       " 'gym attire today was puma singlet adidas shorts and black business socks and leather shoes lucky did not run into any cute girls',\n",
       " 'why won t you show my location',\n",
       " 'no picnic my phone smells like citrus',\n",
       " 'my donkey is sensitive about such comments nevertheless he d and me d be glad to see your mug asap charger is still awol',\n",
       " 'no new csi tonight fml',\n",
       " 'i think my arms are sore from tennis',\n",
       " 'wonders why someone that u like so much can make you so unhappy in a split seccond depressed',\n",
       " 'sleep soon i just hate saying bye and see you tomorrow for the night',\n",
       " 'just got ur newsletter those fares really are unbelievable shame i already booked and paid for mine',\n",
       " 'missin the boo',\n",
       " 'me too itm',\n",
       " 'damn i don t have any chalk my chalkboard is useless',\n",
       " 'had a blast at the getty villa but hates that she s had a sore throat all day it s just getting worse too',\n",
       " 'hey missed ya at the meeting sup mama',\n",
       " 'my tummy hurts i wonder if the hypnosis has anything to do with it if so it s working i get it stop smoking',\n",
       " 'why is it always the fat ones',\n",
       " 'sorry babe my fam annoys me too thankfully they re asleep right now muahaha evil laugh',\n",
       " 'i should have paid more attention when we covered photoshop in my webpage design class in undergrad',\n",
       " 'wednesday my b day don t know what do',\n",
       " 'poor cameron the hills',\n",
       " 'pray for me please the ex is threatening to start sh at my our babies st birthday party what a jerk and i still have a headache',\n",
       " 'hmm do u really enjoy being with him if the problems are too constants u should think things more find someone ulike',\n",
       " 'strider is a sick little puppy',\n",
       " 'so rylee grace wana go steve s party or not sadly since its easter i wnt b able do much but ohh well',\n",
       " 'hey i actually won one of my bracket pools too bad it wasn t the one for money',\n",
       " 'you don t follow me either and i work for you',\n",
       " 'a bad nite for the favorite teams astros and spartans lose the nite out with t w was good']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "testing = df.text[:100]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1a44378a571d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweets %d of %d has been processed\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclean_tweet_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "nums = [0,400000,800000,1200000,1600000]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[4]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[4] ))                                                                  \n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdb com very cool to hear old walt intervie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  target\n",
       "1599995  just woke up having no school is the best feel...       1\n",
       "1599996  thewdb com very cool to hear old walt intervie...       1\n",
       "1599997  are you ready for your mojo makeover ask me fo...       1\n",
       "1599998  happy th birthday to my boo of alll time tupac...       1\n",
       "1599999                               happy charitytuesday       1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['target'] = df.sentiment\n",
    "clean_df.target[clean_df.target == 4] = 1\n",
    "clean_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2226933</th>\n",
       "      <td>a warning sign rt the negativity you bleed out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226934</th>\n",
       "      <td>ff too thank youuu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226935</th>\n",
       "      <td>i just love shumpa that s my girl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226936</th>\n",
       "      <td>the best way to start a day no matter what hap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226937</th>\n",
       "      <td>frenchieswant dtou i m not from french but don...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  target\n",
       "2226933  a warning sign rt the negativity you bleed out...       1\n",
       "2226934                                 ff too thank youuu       1\n",
       "2226935                  i just love shumpa that s my girl       1\n",
       "2226936  the best way to start a day no matter what hap...       1\n",
       "2226937  frenchieswant dtou i m not from french but don...       1"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.to_csv('clean_tweet.csv')\n",
    "csv = 'twitter-datasets/cleaned_train_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2226938 entries, 0 to 2226937\n",
      "Data columns (total 2 columns):\n",
      "text      object\n",
      "target    int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 51.0+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11087</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40228</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text  target\n",
       "6528   NaN       0\n",
       "6722   NaN       0\n",
       "11087  NaN       0\n",
       "19111  NaN       0\n",
       "40228  NaN       0"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df[my_df.isnull().any(axis=1)].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(my_df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       True\n",
       "target    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.isnull().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./trainingandtestdata/training.1600000.processed.noemoticon.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-b9a7714c301f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ISO-8859–1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./trainingandtestdata/training.1600000.processed.noemoticon.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",header=None,encoding=\"ISO-8859–1\")\n",
    "df.iloc[my_df[my_df.isnull().any(axis=1)].index,:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225997 entries, 0 to 2225996\n",
      "Data columns (total 2 columns):\n",
      "text      object\n",
      "target    int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 34.0+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-ad34b68914b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \"\"\"\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \"\"\"\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munigrams_and_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mword_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/wordcloud/tokenization.py\u001b[0m in \u001b[0;36munigrams_and_bigrams\u001b[0;34m(words, normalize_plurals)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     counts_unigrams, standard_form = process_tokens(\n\u001b[0;32m---> 44\u001b[0;31m         words, normalize_plurals=normalize_plurals)\n\u001b[0m\u001b[1;32m     45\u001b[0m     counts_bigrams, standard_form_bigrams = process_tokens(\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/wordcloud/tokenization.py\u001b[0m in \u001b[0;36mprocess_tokens\u001b[0;34m(words, normalize_plurals)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mword_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# get dict of cases for word_lower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mcase_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_lower\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m# increase this case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcase_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neg_tweets = my_df[my_df.target == 0]\n",
    "neg_string = []\n",
    "for t in neg_tweets.text:\n",
    "    neg_string.append(t)\n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in neg_tweets.text[:200]:\n",
    "    if 'love' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = my_df[my_df.target == 1]\n",
    "pos_string = []\n",
    "for t in pos_tweets.text:\n",
    "    pos_string.append(t)\n",
    "pos_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) \n",
    "plt.figure(figsize=(12,10)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(my_df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431979"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix = cvec.transform(my_df[my_df.target == 0].text)\n",
    "pos_doc_matrix = cvec.transform(my_df[my_df.target == 1].text)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix = cvec.transform(my_df.text)\n",
    "my_df[my_df.target == 0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 2181477 entries with 50.84% negative, 49.16% positive\n",
      "Validation set has total 22260 entries with 50.82% negative, 49.18% positive\n",
      "Test set has total 22260 entries with 50.83% negative, 49.17% positive\n"
     ]
    }
   ],
   "source": [
    "x = my_df.text\n",
    "y = my_df.target\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train), (len(x_train[y_train == 0]) / (len(x_train)*1.))*100, (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),(len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,(len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),(len(x_test[y_test == 0]) / (len(x_test)*1.))*100,(len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 57.04%\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "          predicted_positive  predicted_negative\n",
      "positive                9822                1126\n",
      "negative                8436                2876\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.25      0.38     11312\n",
      "          1       0.54      0.90      0.67     10948\n",
      "\n",
      "avg / total       0.63      0.57      0.52     22260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "tbresult = [TextBlob(i).sentiment.polarity for i in x_validation]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]\n",
    "conmat = np.array(confusion_matrix(y_validation, tbpred, labels=[1,0]))\n",
    "confusion = pd.DataFrame(conmat, index=['positive', 'negative'],\n",
    "                         columns=['predicted_positive','predicted_negative'])\n",
    "print(\"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_validation, tbpred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion)\n",
    "print(\"-\"*80)\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_validation, tbpred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print(\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)\n",
    "    return accuracy, train_test_time\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(100000,200001,10000)\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print(classifier)\n",
    "    print(\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Validation result for {} features\".format(n))\n",
    "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit(my_df.text)\n",
    "neg_doc_matrix = cvec.transform(my_df[my_df.target == 0].text)\n",
    "pos_doc_matrix = cvec.transform(my_df[my_df.target == 1].text)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df.to_csv('term_freq_df.csv')\n",
    "csv = 'term_freq_df.csv'\n",
    "term_freq_df = pd.read_csv(csv,index_col=0)\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "a = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))\n",
    "b = text.ENGLISH_STOP_WORDS\n",
    "set(a).issubset(set(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\n",
    "#feature_result_wosw = nfeature_accuracy_checker(stop_words='english')\n",
    "#print(\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\n",
    "#feature_result_ug = nfeature_accuracy_checker()\n",
    "#print(\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Top 10 frequent words)\\n\")\n",
    "#feature_result_wocsw = nfeature_accuracy_checker(stop_words=my_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\n",
    "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR TRIGRAM WITH STOP WORDS\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 100000 features\n",
      "null accuracy: 50.82%\n",
      "accuracy score: 83.72%\n",
      "model is 32.90% more accurate than null accuracy\n",
      "train and test time: 783.96s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 110000 features\n",
      "null accuracy: 50.82%\n",
      "accuracy score: 83.71%\n",
      "model is 32.89% more accurate than null accuracy\n",
      "train and test time: 531.11s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 120000 features\n",
      "null accuracy: 50.82%\n",
      "accuracy score: 83.67%\n",
      "model is 32.85% more accurate than null accuracy\n",
      "train and test time: 701.09s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 130000 features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-347-b86c50a26230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#feature_result_bg = nfeature_accuracy_checker(ngram_range=(1, 2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeature_result_tg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnfeature_accuracy_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-346-23c1baffefa7>\u001b[0m in \u001b[0;36mnfeature_accuracy_checker\u001b[0;34m(vectorizer, n_features, stop_words, ngram_range, classifier)\u001b[0m\n\u001b[1;32m     34\u001b[0m         ])\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation result for {} features\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnfeature_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtt_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchecker_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfeature_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtt_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-346-23c1baffefa7>\u001b[0m in \u001b[0;36maccuracy_summary\u001b[0;34m(pipeline, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnull_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentiment_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_test_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
    "#feature_result_bg = nfeature_accuracy_checker(ngram_range=(1, 2))\n",
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg = nfeature_accuracy_checker(ngram_range=(1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "#nfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "#nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram')\n",
    "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "    confusion = pd.DataFrame(conmat, index=['negative', 'positive'],\n",
    "                         columns=['predicted_negative','predicted_positive'])\n",
    "    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print(\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"-\"*80)\n",
    "    print(\"Confusion Matrix\\n\")\n",
    "    print(confusion)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Classification Report\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['negative','positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_pred(pipeline, x_train, y_train, x_test):\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    \n",
    "    return y_pred;\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tot = x;\n",
    "y_train_tot = y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sea doo pro sea scooter sports with the portab...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shucks well i work all week so now i can t com...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no ma am lol im perfectly fine and not contagi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whenever i fall asleep watching the tv i alway...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  sea doo pro sea scooter sports with the portab...     NaN\n",
       "1  shucks well i work all week so now i can t com...     NaN\n",
       "2            i cant stay away from bug thats my baby     NaN\n",
       "3  no ma am lol im perfectly fine and not contagi...     NaN\n",
       "4  whenever i fall asleep watching the tv i alway...     NaN"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./twitter-datasets/test_tweet.csv\",index_col=0,encoding=\"ISO-8859–1\")\n",
    "x_test = df_test.text.astype('U')  ## Even astype(str) would work\n",
    "\n",
    "print(len(x_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_total = CountVectorizer()\n",
    "cvec_total.fit(pd.Series.append(x_train_tot, x_test))\n",
    "optimal_nfeatures = 10000;\n",
    "ngram_range=(1, 3);\n",
    "vectorizer=cvec_total; \n",
    "n_features=n_features; \n",
    "stop_words=None;\n",
    "classifier=lr;\n",
    "result = []\n",
    "vectorizer.set_params(stop_words=stop_words, max_features=optimal_nfeatures, ngram_range=ngram_range)\n",
    "checker_pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-8b134a93cc31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#neg 0, pos 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchecker_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-341-c1937ecd7348>\u001b[0m in \u001b[0;36mtrain_and_pred\u001b[0;34m(pipeline, x_train, y_train, x_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msentiment_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#neg 0, pos 1\n",
    "y_pred = train_and_pred(checker_pipeline, x_train, y_train, x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "\n",
    "#INVERSE TO GET LOW RESULT ON KAGGLE!!!\n",
    "y_pred[y_pred==1] = -1\n",
    "y_pred[y_pred==0] = 1\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv as CSV\n",
    "ids = np.linspace(1, len(y_pred), num=len(y_pred),dtype=int)\n",
    "print(ids)\n",
    "with open('predictions.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Id', 'Prediction']\n",
    "    writer = CSV.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for r1, r2 in zip(ids, y_pred):\n",
    "        writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
